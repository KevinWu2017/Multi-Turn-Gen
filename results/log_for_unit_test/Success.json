{
  "metadata": {
    "problem": "/home/cpwu/working/code_generation/stanford/framework/third_party/KernelBench/KernelBench/level1/2_Standard_matrix_multiplication_.py",
    "problem_id": "2",
    "sample_id": 0,
    "run_name": "Qwen3",
    "num_rounds": 50
  },
  "1": {
    "context": "You write custom CUDA kernels to replace the pytorch operators in the given architecture to get speedups. \n\n    You have complete freedom to choose the set of operators you want to replace. You may make the decision to replace some operators with custom CUDA kernels and leave others unchanged. You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax). You are only limited by your imagination.\n\n\n        Here's an example to show you the syntax of inline embedding custom CUDA operators in torch: The example given architecture is: \n\n        ``` \n\n        import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, a, b):\n        return a + b\n\n\ndef get_inputs():\n    # randomly generate input tensors based on the model architecture\n    a = torch.randn(1, 128).cuda()\n    b = torch.randn(1, 128).cuda()\n    return [a, b]\n\n\ndef get_init_inputs():\n    # randomly generate tensors required for initialization based on the model architecture\n    return []\n\n        ``` \n\n        The example new arch with custom CUDA kernels looks like this: \n        ```\n        import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for element-wise addition\nelementwise_add_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        out[idx] = a[idx] + b[idx];\n    }\n}\n\ntorch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b) {\n    auto size = a.numel();\n    auto out = torch::zeros_like(a);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);\n\n    return out;\n}\n\"\"\"\n\nelementwise_add_cpp_source = (\n    \"torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);\"\n)\n\n# Compile the inline CUDA code for element-wise addition\nelementwise_add = load_inline(\n    name=\"elementwise_add\",\n    cpp_sources=elementwise_add_cpp_source,\n    cuda_sources=elementwise_add_source,\n    functions=[\"elementwise_add_cuda\"],\n    verbose=True,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\n\nclass ModelNew(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.elementwise_add = elementwise_add\n\n    def forward(self, a, b):\n        return self.elementwise_add.elementwise_add_cuda(a, b)\n\n        ``` \n\n        \n    You are given the following architecture: \n\n    ```\n    import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 1024\nK = 4096\nN = 2048\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n    ```\n    \nOptimize the architecture named Model with custom CUDA operators! Name your optimized output architecture ModelNew. Output the new code in codeblocks. Please generate real code, NOT pseudocode, make sure the code compiles and is fully functional. Just output the new model code, no other text, and NO testing code! \n\n\n\nOptimize the architecture named Model with custom CUDA operators! Name your optimized output architecture ModelNew. Output the new code in codeblocks. Please generate real code, NOT pseudocode, make sure the code compiles and is fully functional. Just output the new model code, no other text, and NO testing code! \n\n",
    "model_reasoning_response": "\nOkay, I need to create a custom CUDA kernel for matrix multiplication to replace the PyTorch matmul operation in the given model. Let me think about how to approach this.\n\nFirst, I remember that matrix multiplication in CUDA is typically done with tiling, where each thread block computes a tile of the output matrix. The threads in a block work together to compute the sum of products for their respective tile. So I'll need to design a kernel that does this.\n\nThe input matrices A (MxK) and B (KxN) will be multiplied to get C (MxN). Each element C[i][j] is the dot product of the i-th row of A and the j-th column of B. To parallelize this, I can assign each thread to compute one element of C. But for efficiency, I should use shared memory to cache the tiles of A and B, which reduces global memory accesses.\n\nLet me outline the steps for the kernel:\n\n1. Determine the thread indices. Each thread will compute C[blockIdx.x * blockDim.x + threadIdx.x][blockIdx.y * blockDim.y + threadIdx.y]. But maybe using a 2D grid and block structure is better here.\n\nWait, in CUDA, threads are organized in a 3D grid and block. For matrix multiplication, a common approach is to have each thread compute one element of the resulting matrix. So for a (M x N) matrix, the grid can be (M/Nt, N/Nt) where Nt is the tile size. But I need to handle cases where M or N isn't a multiple of the tile size.\n\nAlternatively, using a block size of 16x16 or 32x32 threads. Let's pick 16x16 for simplicity. Each block will compute a 16x16 tile of the output matrix. For each tile, the threads in the block will load a 16xK portion of A and a Kx16 portion of B into shared memory. Then perform the dot product using the values in shared memory.\n\nSo the kernel steps would be:\n\n- For each thread in the block, load elements from A and B into shared memory. Each thread may load multiple elements if K is large, requiring multiple iterations (called \"phases\" or \"steps\").\n- Synchronize the threads to ensure shared memory is fully loaded.\n- Compute the dot product by accumulating the products of the elements from the shared memory matrices.\n- Repeat the loading and accumulation for all necessary parts of A and B along the K dimension.\n- Write the final result to the output matrix.\n\nNow, I need to write the CUDA kernel code. Let's start with the kernel function.\n\nThe kernel function will take pointers to A, B, and C, along with the dimensions M, K, N. The matrices are stored in row-major order.\n\nIn the kernel, each thread (x, y) in the block computes C[Row][Col], where Row = blockIdx.y * blockDim.y + threadIdx.y and Col = blockIdx.x * blockDim.x + threadIdx.x. But since the block may be larger than the output matrix, we need to check if Row and Col are within bounds.\n\nFor each iteration along the K dimension (let's say in steps of TILE_SIZE), the thread will load A[Row][k] and B[k][Col] into shared memory. Then perform the multiplication and accumulate.\n\nThe TILE_SIZE for the K dimension could be 16 or 32. Let's choose 16 for now.\n\nSo the code for the kernel would look something like this:\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[16][16];\n    __shared__ float Bs[16][16];\n    int Row = blockIdx.y * blockDim.y + threadIdx.y;\n    int Col = blockIdx.x * blockDim.x + threadIdx.x;\n    int k;\n    float Cvalue = 0.0f;\n    for (k = 0; k < K; k += 16) {\n        // Load tiles of A and B into shared memory\n        int a_idx = Row * K + k + threadIdx.x;\n        int b_idx = k * N + Col + threadIdx.y * N;\n        As[threadIdx.y][threadIdx.x] = (Row < M && k + threadIdx.x < K) ? A[a_idx] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (k + threadIdx.y < K && Col < N) ? B[b_idx] : 0.0f;\n        __syncthreads();\n        // Multiply the tiles\n        for (int i = 0; i < 16; ++i) {\n            Cvalue += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n    if (Row < M && Col < N) {\n        C[Row * N + Col] = Cvalue;\n    }\n}\n\nWait, but the indices for B might be off. Let me think again. Matrix B is KxN. So for B[k][Col], the index in row-major is k*N + Col. But when we load Bs, each thread is responsible for a part. For example, in the loop, for each k, the thread loads B[k + ty][Col], where ty is threadIdx.y. But the index for B would be (k + ty)*N + Col. But in the code above, the b_idx is k*N + Col + threadIdx.y * N. That would be k*N + (Col + ty*N). That's not correct. It should be (k + ty)*N + Col. So the correct index is (k + ty)*N + Col. So the line for Bs should be:\n\nBs[threadIdx.y][threadIdx.x] = ( (k + threadIdx.y) < K && Col < N ) ? B[ (k + threadIdx.y)*N + Col ] : 0.0f;\n\nBut in the code above, the line is:\n\nBs[threadIdx.y][threadIdx.x] = (k + threadIdx.y < K && Col < N) ? B[b_idx] : 0.0f;\n\nwhere b_idx is k * N + Col + threadIdx.y * N = (k + threadIdx.y) * N + Col. Yes, that's correct. So that's right.\n\nNow, the kernel is written. Then, the host function (in C++) will call this kernel. The host function will need to get the dimensions, set up the grid and block sizes.\n\nThe grid dimensions: the output matrix is MxN. Each block processes 16x16 elements. So the grid should have (ceil(N / 16), ceil(M / 16), 1). But in CUDA, the grid is (x, y, z). So for a 2D grid, we can set gridDim.x = ceil(N / blockDim.x), gridDim.y = ceil(M / blockDim.y).\n\nThe block dimensions are (16, 16, 1).\n\nNow, the host function in C++:\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Get the dimensions\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    // Create output tensor\n    auto C = torch::zeros({M, N}, A.options());\n    // Launch the kernel\n    dim3 threads(16, 16);\n    dim3 blocks((N + 15) / 16, (M + 15) / 16);\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    return C;\n}\n\nBut wait, the kernel is written for row-major matrices. However, in PyTorch, tensors are stored in row-major (strided) order, so the code should be correct.\n\nNow, I need to make sure that the kernel is properly compiled with the load_inline function. The C++ code will need to declare the function, and the CUDA code will have the kernel.\n\nPutting it all together, the code for the custom operator would be:\n\nelementwise_add_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[16][16];\n    __shared__ float Bs[16][16];\n    int Row = blockIdx.y * blockDim.y + threadIdx.y;\n    int Col = blockIdx.x * blockDim.x + threadIdx.x;\n    int k;\n    float Cvalue = 0.0f;\n    for (k = 0; k < K; k += 16) {\n        // Load tiles of A and B into shared memory\n        int a_idx = Row * K + k + threadIdx.x;\n        int b_idx = (k + threadIdx.y) * N + Col;\n        As[threadIdx.y][threadIdx.x] = (Row < M && k + threadIdx.x < K) ? A[a_idx] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = ((k + threadIdx.y) < K && Col < N) ? B[b_idx] : 0.0f;\n        __syncthreads();\n        // Multiply the tiles\n        for (int i = 0; i < 16; ++i) {\n            Cvalue += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n    if (Row < M && Col < N) {\n        C[Row * N + Col] = Cvalue;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    auto C = torch::zeros({M, N}, A.options());\n    dim3 threads(16, 16);\n    dim3 blocks((N + 15) / 16, (M + 15) / 16);\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    return C;\n}\n\"\"\"\n\nThen, the C++ header declaration:\n\nelementwise_add_cpp_source = (\n    \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n)\n\nThen, using load_inline to compile the code.\n\nIn the ModelNew class, the forward function calls this custom matmul_cuda function.\n\nBut wait, the original model uses torch.matmul, which is for general matrix multiplication. The custom kernel assumes that A is (M,K) and B is (K,N), and outputs (M,N). The code should handle that.\n\nNow, I need to make sure that the code is correct. For example, the kernel uses 16x16 blocks. Also, the code checks if Row < M and Col < N before writing to C to avoid out-of-bounds writes.\n\nPotential issues: If M, K, or N are not multiples of 16, the code still works because of the bounds checking.\n\nNow, putting all this into the code structure as per the example.\n\nThe final code would include the CUDA source, compile it with load_inline, and then the ModelNew class uses the custom operator.\n\nSo the code would look like this:\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nmatmul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[16][16];\n    __shared__ float Bs[16][16];\n    int Row = blockIdx.y * blockDim.y + threadIdx.y;\n    int Col = blockIdx.x * blockDim.x + threadIdx.x;\n    int k;\n    float Cvalue = 0.0f;\n    for (k = 0; k < K; k += 16) {\n        int a_idx = Row * K + k + threadIdx.x;\n        int b_idx = (k + threadIdx.y) * N + Col;\n        As[threadIdx.y][threadIdx.x] = (Row < M && k + threadIdx.x < K) ? A[a_idx] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = ((k + threadIdx.y) < K && Col < N) ? B[b_idx] : 0.0f;\n        __syncthreads();\n        for (int i = 0; i < 16; ++i) {\n            Cvalue += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n    if (Row < M && Col < N) {\n        C[Row * N + Col] = Cvalue;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    auto C = torch::zeros({M, N}, A.options());\n    dim3 threads(16, 16);\n    dim3 blocks((N + 15) / 16, (M + 15) / 16);\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    return C;\n}\n\"\"\"\n\nmatmul_cpp_source = (\n    \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n)\n\nmatmul = load_inline(\n    name=\"matmul_custom\",\n    cpp_sources=matmul_cpp_source,\n    cuda_sources=matmul_source,\n    functions=[\"matmul_cuda\"],\n    verbose=False,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.matmul = matmul\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        return self.matmul.matmul_cuda(A, B)\n\nWait, but in the example, the elementwise_add is loaded as elementwise_add, and then in the ModelNew, it's accessed via self.elementwise_add.elementwise_add_cuda. Similarly, here, the matmul is loaded as matmul, and the function is matmul_cuda. So the code should be correct.\n\nBut I need to make sure that the function is called correctly. The load_inline returns a module, and the functions are attributes of that module. So in the forward, self.matmul.matmul_cuda(A, B) is correct.\n\nNow, the code should compile and run. However, there might be some issues with the CUDA code. For example, the kernel may not be optimized for all cases, but it's a basic implementation. Also, the code assumes that the input tensors are 2D and of the correct shapes.\n\nAnother thing to consider is that the original model uses torch.matmul, which can handle different input shapes and types. The custom kernel is specific to 2D tensors with float32 data. So the code may need to add checks or handle different data types, but for the given problem, assuming that the inputs are as per the example (float32, 2D), it's acceptable.\n\nSo the final code should be as above.\n",
    "model_response": "\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nmatmul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[16][16];\n    __shared__ float Bs[16][16];\n    int Row = blockIdx.y * blockDim.y + threadIdx.y;\n    int Col = blockIdx.x * blockDim.x + threadIdx.x;\n    int k;\n    float Cvalue = 0.0f;\n    for (k = 0; k < K; k += 16) {\n        int a_idx = Row * K + k + threadIdx.x;\n        int b_idx = (k + threadIdx.y) * N + Col;\n        As[threadIdx.y][threadIdx.x] = (Row < M && k + threadIdx.x < K) ? A[a_idx] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = ((k + threadIdx.y) < K && Col < N) ? B[b_idx] : 0.0f;\n        __syncthreads();\n        for (int i = 0; i < 16; ++i) {\n            Cvalue += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n    if (Row < M && Col < N) {\n        C[Row * N + Col] = Cvalue;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    auto C = torch::zeros({M, N}, A.options());\n    dim3 threads(16, 16);\n    dim3 blocks((N + 15) / 16, (M + 15) / 16);\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    return C;\n}\n\"\"\"\n\nmatmul_cpp_source = (\n    \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n)\n\nmatmul = load_inline(\n    name=\"matmul_custom\",\n    cpp_sources=matmul_cpp_source,\n    cuda_sources=matmul_source,\n    functions=[\"matmul_cuda\"],\n    verbose=False,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.matmul = matmul\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        return self.matmul.matmul_cuda(A, B)\n```",
    "kernel_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nmatmul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[16][16];\n    __shared__ float Bs[16][16];\n    int Row = blockIdx.y * blockDim.y + threadIdx.y;\n    int Col = blockIdx.x * blockDim.x + threadIdx.x;\n    int k;\n    float Cvalue = 0.0f;\n    for (k = 0; k < K; k += 16) {\n        int a_idx = Row * K + k + threadIdx.x;\n        int b_idx = (k + threadIdx.y) * N + Col;\n        As[threadIdx.y][threadIdx.x] = (Row < M && k + threadIdx.x < K) ? A[a_idx] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = ((k + threadIdx.y) < K && Col < N) ? B[b_idx] : 0.0f;\n        __syncthreads();\n        for (int i = 0; i < 16; ++i) {\n            Cvalue += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n    if (Row < M && Col < N) {\n        C[Row * N + Col] = Cvalue;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    auto C = torch::zeros({M, N}, A.options());\n    dim3 threads(16, 16);\n    dim3 blocks((N + 15) / 16, (M + 15) / 16);\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    return C;\n}\n\"\"\"\n\nmatmul_cpp_source = (\n    \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n)\n\nmatmul = load_inline(\n    name=\"matmul_custom\",\n    cpp_sources=matmul_cpp_source,\n    cuda_sources=matmul_source,\n    functions=[\"matmul_cuda\"],\n    verbose=False,\n    extra_cflags=[\"\"],\n    extra_ldflags=[\"\"],\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.matmul = matmul\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        return self.matmul.matmul_cuda(A, B)",
    "feedback": "",
    "eval_result": {
      "compiled": true,
      "correctness": true,
      "metadata": {
        "hardware": "NVIDIA A100 80GB PCIe",
        "device": "cuda:0",
        "correctness_trials": "(5 / 5)"
      },
      "runtime": 3.83,
      "runtime_stats": {
        "mean": 3.83,
        "std": 0.0824,
        "min": 3.72,
        "max": 4.09,
        "num_trials": 100,
        "hardware": "NVIDIA A100 80GB PCIe",
        "device": "cuda:0"
      }
    },
    "profiler_result": "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nmatmul_kernel(float const*, float const*, float*, in...         0.00%       0.000us         0.00%       0.000us       0.000us     365.410ms        99.85%     365.410ms       3.806ms            96  \nvoid at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     548.262us         0.15%     548.262us       5.771us            95  \n                                       cudaLaunchKernel         0.17%     612.816us         0.17%     612.816us       3.295us       0.000us         0.00%       0.000us       0.000us           186  \n                                  cudaDeviceSynchronize        99.83%     365.864ms        99.83%     365.864ms     365.864ms       0.000us         0.00%       0.000us       0.000us             1  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 366.477ms\nSelf CUDA time total: 365.958ms\n"
  },
  "2": {}
}